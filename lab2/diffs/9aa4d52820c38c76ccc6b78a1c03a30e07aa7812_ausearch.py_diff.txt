@@ -5,8 +5,8 @@ from copy import copy
 from dataclasses import dataclass
 from functools import lru_cache
 from typing import Any, Dict, Generic, List, Optional, TypeVar, Union
+from sys import float_info
 
-import cipheycore
 from loguru import logger
 
 from ciphey.iface import (
@@ -100,11 +100,130 @@ class Node:
             return [self.level]
         return self.parent.source.get_path() + [self.level]
 
+@dataclass
+class AusearchEdge:
+    # TODO: This is just CrackInfo with failure probability added...
+    success_probability: float
+    failure_probability: float
+    success_time: float
+    failure_time: float
+
+    def __init__(self, success_probability, success_time, failure_time):
+        self.success_probability = success_probability
+        self.failure_probability = 1.0 - success_probability
+        self.success_time = success_time
+        self.failure_time = failure_time
+
+@dataclass
+class AusearchResult:
+    weight: float
+    index: int
 
 def convert_edge_info(info: CrackInfo):
-    return cipheycore.ausearch_edge(
-        info.success_likelihood, info.success_runtime, info.failure_runtime
-    )
+    return 1
+
+# Equivalent to CPP ausearch_minimise
+def minimise_edges(edges) -> AusearchResult: 
+    weight = minimise_edges_impl(edges)
+    index = len(edges) # TODO: Is this ever different? C++ code seems to just count the number of elements...but in a really cursed way
+    return AusearchResult(weight, index)
+
+# Equivalent to cpp minimise_edges
+def minimise_edges_impl(edges) -> float:
+    num_edges = len(edges)
+    if num_edges == 0:
+        return float('NaN')
+    elif num_edges == 1:
+        return calculate_weight(edges)
+
+    # NOTE: Comment originally from C++ code. 
+    # TODO: Validate if this is actually true or not in Python
+    # It turns out that it is faster to optimise for antiweight (weight in reverse)
+    #
+    # This is because weight is iteratively calculated from the end, 
+    # and most machines run faster when iterating *forwards*
+
+    # First, we calculate a lower bound on the weight
+    weight = 0.0
+    old_weight = calculate_antiweight(edges)
+    while True:
+        remaining_weight = old_weight
+
+        # Now, iterating down the edge list, trying to find the minimising value
+        for current_pos, _ in enumerate(edges[:-1]):
+            max_remaining_weight = float_info.min
+            max_pos = -1
+
+            for i, target in enumerate(edges[current_pos:]):
+                edge_remaining_weight = remaining_weight
+                # We didn't succeed
+                edge_remaining_weight -= (target.success_probability * target.success_time)
+                # Expand the rest of the weight
+                if (target.failure_probability == 0):
+                    # If we cannot fail, then there can be no remaining antiweight
+                    edge_remaining_weight = 0
+                else:
+                    edge_remaining_weight /= target.failure_probability
+                
+                if edge_remaining_weight > max_remaining_weight:
+                    max_remaining_weight = edge_remaining_weight
+                    max_pos = i
+            
+            # Swap edges[current_pos] with edges[max]
+            edges[current_pos], edges[max_pos] = edges[max_pos], edges[current_pos]
+            remaining_weight = max_remaining_weight
+        
+        weight = calculate_antiweight(edges)
+
+        while True:
+            tmp_weight = weight
+
+            for pos, _ in enumerate(edges[:-2]):
+                brute_edges(edges, pos)
+            
+            weight = brute_edges(edges, len(edges)-2)
+            if (weight == tmp_weight):
+                # No progress made; exit the loop
+                break
+
+        if weight == old_weight:
+            # No progress made; exit the loop
+            break
+        
+        old_weight = weight
+    
+    # antiweight -> weight
+    edges.reverse()
+    return calculate_weight(edges)
+
+def calculate_weight(edges):
+    weight = 0.0
+    for edge in reversed(edges):
+        weight = (edge.success_probability * edge.success_time) + ((edge.failure_probability) * (edge.failure_time + weight))
+    return weight
+
+def calculate_antiweight(edges):
+    # Original comment from C++ code: "Iterating forward is *way* faster"
+    # TODO: Verify that this actually applies in Python
+    weight = 0.0
+    for edge in edges:
+        weight = (edge.success_probability * edge.success_time) + ((edge.failure_probability) * (edge.failure_time + weight))
+    return weight
+
+def brute_edges(edges, target: int):
+    best_weight = calculate_antiweight(edges)
+    # Triangle swaps
+    for i in range(target+1, len(edges)):
+        edges[i], edges[target] = edges[target], edges[i]
+        new_weight = calculate_antiweight(edges)
+
+        if new_weight < best_weight:
+            best_weight = new_weight
+        else:
+            # Swap back
+            edges[i], edges[target] = edges[target], edges[i]
+
+    return best_weight
 
 
 @dataclass
@@ -113,7 +232,7 @@ class Edge:
     route: Union[Cracker, Decoder]
     dest: Optional[Node] = None
     # Info is not filled in for Decoders
-    info: Optional[cipheycore.ausearch_edge] = None
+
 
 
 PriorityType = TypeVar("PriorityType")
@@ -186,7 +305,7 @@ class AuSearch(Searcher):
         for i in self.get_crackers_for(type(res)):
             inst = self._config()(i)
             additional_work.append(
-                Edge(source=node, route=inst, info=convert_edge_info(inst.getInfo(res)))
+                Edge(source=node, route=inst)
             )
         priority = min(node.depth, self.priority_cap)
         if self.invert_priority:
@@ -246,7 +365,6 @@ class AuSearch(Searcher):
                     break
                 # Get the highest level result
                 chunk = self.work.get_work_chunk()
-                infos = [i.info for i in chunk]
                 # Work through all of this level's results
                 while len(chunk) != 0:
                     max_depth = 0
@@ -259,15 +377,13 @@ class AuSearch(Searcher):
                     #     chunk += self.work.get_work_chunk()
                     #     infos = [i.info for i in chunk]
 
-                    logger.trace(f"{len(infos)} remaining on this level")
-                    step_res = cipheycore.ausearch_minimise(infos)
-                    edge: Edge = chunk.pop(step_res.index)
+                    step_res = 0.1
+                    edge: Edge = chunk.pop(0)
                     logger.trace(
-                        f"Weight is currently {step_res.weight} "
+                        f"Weight is currently {0} "
                         f"when we pick {type(edge.route).__name__.lower()} "
                         f"with depth {edge.source.depth}"
                     )
-                    del infos[step_res.index]
 
                     # Expand the node
                     res = edge.route(edge.source.level.result.value)
@@ -337,4 +453,4 @@ class AuSearch(Searcher):
                 desc="Sets the maximum depth before we give up ordering items.",
                 default="2",
             ),
-        }
+        }
\ No newline at end of file
